{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture \n!pip install trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:35:21.006067Z","iopub.execute_input":"2025-04-17T09:35:21.006481Z","iopub.status.idle":"2025-04-17T09:36:59.820167Z","shell.execute_reply.started":"2025-04-17T09:35:21.006442Z","shell.execute_reply":"2025-04-17T09:36:59.818366Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### **Implementing GRPO in TRL**\n\n---\n\n#### **GRPO là gì?**\nGRPO (Group Relative Policy Optimization) là một thuật toán tăng cường đặc biệt trong dòng RLHF, giúp mô hình học **từ sự so sánh giữa các nhóm phản hồi (completions)** – thay vì đánh giá từng phản hồi riêng lẻ.\n\n---\n\n### **Các khái niệm chính trong GRPO**\n\n1. **Group Formation**:  \n   Mô hình sinh ra **nhiều completions cho mỗi prompt**, tạo thành một nhóm để so sánh.\n\n2. **Preference Learning**:  \n   Một **hàm reward** được định nghĩa để **so sánh và xếp hạng các phản hồi** trong nhóm → từ đó điều chỉnh policy.\n\n3. **Training Configuration**:  \n   Việc huấn luyện được điều khiển thông qua `GRPOConfig`.\n\n---\n\n### **Để triển khai GRPO với TRL, cần:**\n\n| Bước | Mô tả |\n|------|------|\n| 1️⃣ | Định nghĩa dataset chứa **prompt** |\n| 2️⃣ | Định nghĩa **reward function**: đầu vào là list completions, đầu ra là list điểm reward |\n| 3️⃣ | Tạo `GRPOConfig` để cấu hình quá trình huấn luyện |\n| 4️⃣ | Sử dụng `GRPOTrainer` để huấn luyện mô hình |","metadata":{}},{"cell_type":"code","source":"from trl import GRPOTrainer, GRPOConfig\nfrom datasets import load_dataset\n\n# 1. Load your dataset\ndataset = load_dataset(\"your_dataset\", split=\"train\")\n\n\n# 2. Define a simple reward function\ndef reward_func(completions, **kwargs):\n    \"\"\"Example: Reward longer completions\"\"\"\n    return [float(len(completion)) for completion in completions]\n\n\n# 3. Configure training\ntraining_args = GRPOConfig(\n    output_dir=\"output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    logging_steps=10,\n)\n\n# 4. Initialize and train\ntrainer = GRPOTrainer(\n    model=\"your_model\",  # e.g. \"Qwen/Qwen2-0.5B-Instruct\"\n    args=training_args,\n    train_dataset=dataset,\n    reward_funcs=reward_func,\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Key Components**\n\n## **1. Dataset Format**\n\nDataset cần phải chứa **prompt** mà mô hình sẽ tạo ra câu trả lời. GRPO Trainer sẽ tạo ra nhiều câu trả lời cho mỗi **prompt** và sử dụng **reward function** để so sánh chúng.\n\n## **2. Reward Function**\n\nReward function định hướng việc học. Bạn sẽ cần viết một hàm nhận vào một list completions và trả ra list điểm số (reward).\n\n","metadata":{}},{"cell_type":"code","source":"# Example 1: Reward based on completion length\ndef reward_length(completions, **kwargs):\n    return [float(len(completion)) for completion in completions]\n\n\n# Example 2: Reward based on matching a pattern\nimport re\n\ndef reward_format(completions, **kwargs):\n    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n    return [1.0 if re.match(pattern, c) else 0.0 for c in completions]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:41:52.222339Z","iopub.execute_input":"2025-04-17T09:41:52.225616Z","iopub.status.idle":"2025-04-17T09:41:52.242685Z","shell.execute_reply.started":"2025-04-17T09:41:52.225538Z","shell.execute_reply":"2025-04-17T09:41:52.240226Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## **3. `GRPOConfig`**\n\n| Tham số | Ý nghĩa | Gợi ý |\n|---------|--------|--------|\n| `num_generation` | Số completions được sinh ra/mỗi prompt | **4–8** cho bài toán nhẹ, **8–16** nếu bài toán phức tạp (như NL → FOL reasoning) |\n| `use_vllm` | Dùng backend VLLM để tăng tốc sinh text | Bật nếu bạn dùng GPU lớn hoặc inference nhiều |\n| `per_device_train_batch_size` | Nên bằng `num_generation` nếu có thể | Đảm bảo mỗi batch chứa đủ nhóm phản hồi |\n","metadata":{}},{"cell_type":"code","source":"training_args = GRPOConfig(\n    # Essential parameters\n    output_dir=\"output\",\n    num_train_epochs=3,\n    num_generation=4,  # Number of completions to generate for each prompt\n    per_device_train_batch_size=4,  # We want to get all generations in one device batch\n    # Optional but useful\n    gradient_accumulation_steps=2,\n    learning_rate=1e-5,\n    logging_steps=10,\n    # GRPO specific (optional)\n    use_vllm=True,  # Speed up generation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T09:42:02.532862Z","iopub.execute_input":"2025-04-17T09:42:02.533430Z","iopub.status.idle":"2025-04-17T09:42:02.641031Z","shell.execute_reply.started":"2025-04-17T09:42:02.533379Z","shell.execute_reply":"2025-04-17T09:42:02.638945Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3514907411.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_args = GRPOConfig(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Essential parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of completions to generate for each prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'GRPOConfig' is not defined"],"ename":"NameError","evalue":"name 'GRPOConfig' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"### **Tips for Success**\n\n1. **Quản lý bộ nhớ:** Điều chỉnh `per_device_train_batch_size` và `gradient_accumulation_steps` tùy theo dung lượng GPU.\n2. **Tăng tốc:** Bật `use_vllm=True` nếu mô hình của bạn được hỗ trợ, để tăng tốc độ sinh văn bản.\n3. **Giám sát:** Theo dõi các chỉ số trong quá trình huấn luyện:\n  - `reward`: Phần thưởng trung bình của các câu sinh ra.\n  - `reward_std`: Độ lệch chuẩn của phần thưởng giữa các nhóm.\n  - `kl`: Độ lệch KL so với mô hình tham chiếu.","metadata":{}},{"cell_type":"markdown","source":"### **Reward Function Design**\n1. **Length-Based Rewards**: Một trong những reward dễ thiết kế nhất là dựa trên độ dài. Có thể thiết kế để phạt các mà phản hồi quá ngắn hoặc quá dài.\n2. **Rule-Based Rewards for Verifiable Tasks**: Đối với những nhiệm vụ có câu trả lời đúng khách quan (như mã hóa hoặc tính toán), có thể triển khai phần thưởng dựa trên quy tắc (ví dụ, nếu trả lời đúng +1, sai +0).\n3. **Format-Based Rewards**: Có thể thiết kế phần thưởng dựa trên format của câu trả lời (ví dụ, nếu phản hồi có <think> token và thỏa mãn +1).","metadata":{}}]}